{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe file 'venv/lib/python3.10/site-packages/typing_extensions.py' seems to be overriding built in modules and interfering with the startup of the kernel. Consider renaming the file and starting the kernel again.\n",
      "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresOverridingBuiltInModules'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "!pip install supervision "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe file 'venv/lib/python3.10/site-packages/typing_extensions.py' seems to be overriding built in modules and interfering with the startup of the kernel. Consider renaming the file and starting the kernel again.\n",
      "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresOverridingBuiltInModules'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "from datetime import datetime\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "from scipy.spatial import KDTree\n",
    "from transformers import DPTFeatureExtractor, DPTForDepthEstimation\n",
    "from ultralytics import YOLO\n",
    "import supervision as sv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe file 'venv/lib/python3.10/site-packages/typing_extensions.py' seems to be overriding built in modules and interfering with the startup of the kernel. Consider renaming the file and starting the kernel again.\n",
      "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresOverridingBuiltInModules'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "!pip uninstall typing_extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SceneAnalyzer:\n",
    "    def __init__(self, depth_model_name=\"Intel/dpt-large\", yolo_model_name=\"yolov8x.pt\"):\n",
    "        \"\"\"Initialize models and parameters\"\"\"\n",
    "        self.depth_feature_extractor = DPTFeatureExtractor.from_pretrained(depth_model_name)\n",
    "        self.depth_model = DPTForDepthEstimation.from_pretrained(depth_model_name)\n",
    "        self.yolo_model = YOLO(yolo_model_name, classes=[])\n",
    "        self.scaling_factor = 10\n",
    "        self.safety_threshold = 6  # minimum safe distance in feet\n",
    "        self.historical_data = []\n",
    "\n",
    "    def detect_objects(self, image_path):\n",
    "        \"\"\"\n",
    "        Perform object detection on the image\n",
    "\n",
    "        Args:\n",
    "            image_path (str): Path to the image file\n",
    "\n",
    "        Returns:\n",
    "            list: List of YOLO detection results\n",
    "        \"\"\"\n",
    "        results = self.yolo_model(image_path)\n",
    "        return results\n",
    "\n",
    "    def estimate_depth(self, image):\n",
    "        \"\"\"Estimate depth map from image and return both depth map and scale factors\"\"\"\n",
    "        depth_inputs = self.depth_feature_extractor(images=image, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            depth_outputs = self.depth_model(**depth_inputs)\n",
    "            predicted_depth = depth_outputs.predicted_depth.squeeze().cpu().numpy()\n",
    "\n",
    "        depth_min, depth_max = predicted_depth.min(), predicted_depth.max()\n",
    "        normalized_depth = (predicted_depth - depth_min) / (depth_max - depth_min)\n",
    "\n",
    "        # Calculate scale factors between original image and depth map\n",
    "        original_height, original_width = np.array(image).shape[:2]\n",
    "        depth_height, depth_width = normalized_depth.shape\n",
    "        height_scale = depth_height / original_height\n",
    "        width_scale = depth_width / original_width\n",
    "\n",
    "        return normalized_depth * self.scaling_factor, (height_scale, width_scale)\n",
    "\n",
    "    def draw_annotations(self, image, detections, names, confidences):\n",
    "        \"\"\"Draw bounding boxes and labels using cv2 with enhanced visibility\"\"\"\n",
    "        annotated_image = image.copy()\n",
    "\n",
    "        for i, (box, class_id, conf) in enumerate(zip(detections.xyxy, detections.class_id, detections.confidence)):\n",
    "            x1, y1, x2, y2 = map(int, box)\n",
    "\n",
    "            # Use different colors for different classes\n",
    "            color = (0, 255, 0) if names[class_id] == 'chair' else (0, 0, 255)\n",
    "\n",
    "            # Draw rectangle with thicker lines\n",
    "            cv2.rectangle(annotated_image, (x1, y1), (x2, y2), color, 3)\n",
    "\n",
    "            # Create more detailed label\n",
    "            label = f\"{names[class_id]} ({conf:.3f})\"\n",
    "\n",
    "            # Make text background and font larger\n",
    "            font_scale = 0.7\n",
    "            thickness = 2\n",
    "            (label_width, label_height), baseline = cv2.getTextSize(\n",
    "                label, cv2.FONT_HERSHEY_SIMPLEX, font_scale, thickness\n",
    "            )\n",
    "\n",
    "            # Draw label background\n",
    "            cv2.rectangle(\n",
    "                annotated_image,\n",
    "                (x1, y1 - label_height - baseline - 10),\n",
    "                (x1 + label_width + 10, y1),\n",
    "                color,\n",
    "                -1,\n",
    "            )\n",
    "\n",
    "            # Draw label text in white for better contrast\n",
    "            cv2.putText(\n",
    "                annotated_image,\n",
    "                label,\n",
    "                (x1 + 5, y1 - baseline - 5),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                font_scale,\n",
    "                (255, 255, 255),\n",
    "                thickness,\n",
    "            )\n",
    "\n",
    "            # Print detection details to console\n",
    "            print(f\"Detected {names[class_id]} with {conf:.3f} confidence at position {x1},{y1},{x2},{y2}\")\n",
    "\n",
    "        return annotated_image\n",
    "\n",
    "    def analyze_scene(self, image_path):\n",
    "        \"\"\"Main method to analyze the scene with enhanced debugging\"\"\"\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        image_np = np.array(image)\n",
    "        estimated_distance, scale_factors = self.estimate_depth(image)\n",
    "        yolo_results = self.detect_objects(image_path)[0]\n",
    "\n",
    "        print(\"\\nDetection Results:\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "        # Print all detections with confidence scores\n",
    "        for i, (box, cls, conf) in enumerate(zip(yolo_results.boxes.xyxy, yolo_results.boxes.cls, yolo_results.boxes.conf)):\n",
    "            print(f\"Object {i+1}:\")\n",
    "            print(f\"  Class: {yolo_results.names[int(cls)]}\")\n",
    "            print(f\"  Confidence: {conf:.3f}\")\n",
    "            print(f\"  Bounding Box: {box.tolist()}\")\n",
    "            print()\n",
    "\n",
    "        # Convert YOLO results to supervision detections\n",
    "        detections = sv.Detections(\n",
    "            xyxy=yolo_results.boxes.xyxy.cpu().numpy(),\n",
    "            confidence=yolo_results.boxes.conf.cpu().numpy(),\n",
    "            class_id=yolo_results.boxes.cls.cpu().numpy().astype(int)\n",
    "        )\n",
    "\n",
    "    def calculate_spatial_relationships(self, detections, names):\n",
    "        \"\"\"Calculate spatial relationships between objects\"\"\"\n",
    "        relationships = []\n",
    "        if len(detections.xyxy) > 1:\n",
    "            # Calculate center points of all bounding boxes\n",
    "            centers = np.array([\n",
    "                [(box[0] + box[2])/2, (box[1] + box[3])/2]\n",
    "                for box in detections.xyxy\n",
    "            ])\n",
    "\n",
    "            tree = KDTree(centers)\n",
    "            for i, pos in enumerate(centers):\n",
    "                distances, indices = tree.query(pos.reshape(1, -1), k=2)\n",
    "                if len(indices[0]) > 1:\n",
    "                    nearest_neighbor = indices[0][1]\n",
    "                    # Use object names instead of class IDs\n",
    "                    relationships.append({\n",
    "                        'object1': names[detections.class_id[i]],\n",
    "                        'object2': names[detections.class_id[nearest_neighbor]],\n",
    "                        'distance': float(distances[0][1]),\n",
    "                        'relative_position': 'left' if centers[i][0] < centers[nearest_neighbor][0] else 'right'\n",
    "                    })\n",
    "        return relationships\n",
    "\n",
    "    def generate_safety_alerts(self, detections, names):\n",
    "        \"\"\"Generate safety alerts based on object proximity\"\"\"\n",
    "        alerts = []\n",
    "        processed_pairs = set()  # To avoid duplicate alerts\n",
    "\n",
    "        for i in range(len(detections.xyxy)):\n",
    "            for j in range(i + 1, len(detections.xyxy)):\n",
    "                # Create a unique pair identifier\n",
    "                pair_id = tuple(sorted([i, j]))\n",
    "                if pair_id in processed_pairs:\n",
    "                    continue\n",
    "\n",
    "                processed_pairs.add(pair_id)\n",
    "\n",
    "                box1 = detections.xyxy[i]\n",
    "                box2 = detections.xyxy[j]\n",
    "                center1 = [(box1[0] + box1[2])/2, (box1[1] + box1[3])/2]\n",
    "                center2 = [(box2[0] + box2[2])/2, (box2[1] + box2[3])/2]\n",
    "                dist = np.sqrt((center1[0] - center2[0])**2 + (center1[1] - center2[1])**2)\n",
    "\n",
    "                if dist < self.safety_threshold * 100:  # Convert feet to pixels\n",
    "                    alerts.append({\n",
    "                        'severity': 'HIGH' if dist < self.safety_threshold * 50 else 'MEDIUM',\n",
    "                        'message': f\"Safety concern: {names[detections.class_id[i]]} and {names[detections.class_id[j]]} are too close\",\n",
    "                        'distance': dist / 100  # Convert to feet\n",
    "                    })\n",
    "        return alerts\n",
    "\n",
    "    def analyze_scene(self, image_path):\n",
    "        \"\"\"Main method to analyze the scene\"\"\"\n",
    "        # Load and process image\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        image_np = np.array(image)\n",
    "        estimated_distance, scale_factors = self.estimate_depth(image)\n",
    "        yolo_results = self.detect_objects(image_path)[0]  # Get first result\n",
    "\n",
    "        # Convert YOLO results to supervision detections\n",
    "        detections = sv.Detections(\n",
    "            xyxy=yolo_results.boxes.xyxy.cpu().numpy(),\n",
    "            confidence=yolo_results.boxes.conf.cpu().numpy(),\n",
    "            class_id=yolo_results.boxes.cls.cpu().numpy().astype(int)\n",
    "        )\n",
    "\n",
    "        # Draw annotations using our custom method\n",
    "        annotated_image = self.draw_annotations(\n",
    "            image_np,\n",
    "            detections,\n",
    "            yolo_results.names,\n",
    "            detections.confidence\n",
    "        )\n",
    "\n",
    "        # Process detection results\n",
    "        object_data = []\n",
    "        height_scale, width_scale = scale_factors\n",
    "\n",
    "        for i, (xyxy, confidence, class_id) in enumerate(zip(detections.xyxy, detections.confidence, detections.class_id)):\n",
    "            x1, y1, x2, y2 = map(int, xyxy)\n",
    "\n",
    "            # Scale coordinates for depth map\n",
    "            scaled_x1 = int(x1 * width_scale)\n",
    "            scaled_y1 = int(y1 * height_scale)\n",
    "            scaled_x2 = int(x2 * width_scale)\n",
    "            scaled_y2 = int(y2 * height_scale)\n",
    "\n",
    "            # Ensure coordinates are within bounds\n",
    "            scaled_x1 = min(max(0, scaled_x1), estimated_distance.shape[1] - 1)\n",
    "            scaled_y1 = min(max(0, scaled_y1), estimated_distance.shape[0] - 1)\n",
    "            scaled_x2 = min(max(0, scaled_x2), estimated_distance.shape[1] - 1)\n",
    "            scaled_y2 = min(max(0, scaled_y2), estimated_distance.shape[0] - 1)\n",
    "\n",
    "            # Calculate object distance using scaled coordinates\n",
    "            object_depth = estimated_distance[scaled_y1:scaled_y2, scaled_x1:scaled_x2]\n",
    "            object_distance = float(np.mean(object_depth)) if object_depth.size > 0 else None\n",
    "\n",
    "            # Store object data\n",
    "            object_data.append({\n",
    "                'type': yolo_results.names[class_id],\n",
    "                'confidence': float(confidence),\n",
    "                'distance': object_distance,\n",
    "                'position': {'x1': x1, 'y1': y1, 'x2': x2, 'y2': y2}\n",
    "            })\n",
    "\n",
    "        # Calculate additional analysis using object names instead of class IDs\n",
    "        spatial_relationships = self.calculate_spatial_relationships(detections, yolo_results.names)\n",
    "        safety_alerts = self.generate_safety_alerts(detections, yolo_results.names)\n",
    "\n",
    "        # Store historical data (without JSON serialization)\n",
    "        analysis_data = {\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'objects': object_data,\n",
    "            'relationships': spatial_relationships,\n",
    "            'alerts': safety_alerts\n",
    "        }\n",
    "        self.historical_data.append(analysis_data)\n",
    "\n",
    "        # Create basic visualizations\n",
    "        fig = plt.figure(figsize=(15, 5))\n",
    "\n",
    "        # Original image with annotations\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(cv2.cvtColor(annotated_image, cv2.COLOR_BGR2RGB))\n",
    "        plt.title(\"Object Detection with Confidence Scores\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        # Depth map\n",
    "        plt.subplot(1, 2, 2)\n",
    "        depth_plot = plt.imshow(estimated_distance, cmap=\"inferno\")\n",
    "        plt.colorbar(depth_plot, label=\"Distance (Feet)\")\n",
    "        plt.title(\"Depth Map\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        return {\n",
    "            'annotated_image': annotated_image,\n",
    "            'depth_map': estimated_distance,\n",
    "            'object_data': object_data,\n",
    "            'spatial_relationships': spatial_relationships,\n",
    "            'safety_alerts': safety_alerts,\n",
    "            'figure': fig\n",
    "        }\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    analyzer = SceneAnalyzer()\n",
    "\n",
    "    image_paths = [\n",
    "        \"insideb.jpeg\"\n",
    "    ]\n",
    "\n",
    "    for image_path in image_paths:\n",
    "        print(f\"\\nAnalyzing {image_path}...\")\n",
    "        results = analyzer.analyze_scene(image_path)\n",
    "\n",
    "        # Enhanced printing of object details\n",
    "        print(\"\\nDetected Objects Summary:\")\n",
    "        print(\"-\" * 50)\n",
    "        for obj in results['object_data']:\n",
    "            print(f\"Type: {obj['type']}\")\n",
    "            print(f\"Confidence: {obj['confidence']:.3f}\")\n",
    "            print(f\"Estimated Distance: {obj['distance']:.2f} feet\")\n",
    "            print(\"-\" * 30)\n",
    "\n",
    "        if results['safety_alerts']:\n",
    "            print(\"\\nSafety Alerts:\")\n",
    "            for alert in results['safety_alerts']:\n",
    "                print(f\"- {alert['severity']}: {alert['message']} ({alert['distance']:.2f} feet)\")\n",
    "\n",
    "        if results['spatial_relationships']:\n",
    "            print(\"\\nSpatial Relationships:\")\n",
    "            for rel in results['spatial_relationships']:\n",
    "                print(f\"- {rel['object1']} is {rel['relative_position']} of {rel['object2']}\")\n",
    "\n",
    "        plt.figure(results['figure'].number)\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
